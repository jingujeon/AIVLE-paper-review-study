Metadata-Version: 2.4
Name: liquid
Version: 1.0.0
Summary: Liquid: Language Models are Scalable and Unified Multi-modal Generators
Project-URL: Homepage, https://foundationvision.github.io/Liquid/
Project-URL: Bug Tracker, https://github.com/FoundationVision/Liquid/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch==2.1.0
Requires-Dist: torchvision==0.16.0
Requires-Dist: transformers==4.39.2
Requires-Dist: tokenizers==0.15.1
Requires-Dist: sentencepiece==0.1.99
Requires-Dist: shortuuid
Requires-Dist: accelerate==0.27.2
Requires-Dist: peft==0.4.0
Requires-Dist: bitsandbytes==0.41.0
Requires-Dist: pydantic<2,>=1
Requires-Dist: markdown2[all]
Requires-Dist: numpy==1.26.4
Requires-Dist: scikit-learn==1.2.2
Requires-Dist: requests
Requires-Dist: httpx==0.24.0
Requires-Dist: uvicorn
Requires-Dist: fastapi
Requires-Dist: einops==0.6.1
Requires-Dist: einops-exts==0.0.4
Provides-Extra: train
Requires-Dist: deepspeed==0.13.0; extra == "train"
Requires-Dist: ninja; extra == "train"
Provides-Extra: build
Requires-Dist: build; extra == "build"
Requires-Dist: twine; extra == "build"
Dynamic: license-file

<div align="center">
<h1>Liquid: Language Models are Scalable and Unified <br> Multi-modal Generators</h1>

[**Junfeng Wu**](https://wjf5203.github.io/)<sup>1,2</sup> 路 [**Yi Jiang**](https://enjoyyi.github.io/)<sup>2&dagger;</sup> 路 [**Chuofan Ma**](https://machuofan.github.io/)<sup>2,3</sup>
<br>
[**Yuliang Liu**](https://openreview.net/profile?id=~Yuliang_Liu2)<sup>1</sup> 路 [**Hengshuang Zhao**](https://hszhao.github.io/)<sup>3</sup>
<br>
[**Zehuan Yuan**](https://shallowyuan.github.io/)<sup>2</sup> 路 [**Song Bai**](https://songbai.site/)<sup>2*</sup> 路 [**Xiang Bai**](http://vlrlab.aia.hust.edu.cn/)<sup>1*</sup>

<sup>1</sup>HUST&emsp;&emsp;&emsp;<sup>2</sup>ByteDance&emsp;&emsp;&emsp;<sup>3</sup>HKU
<br>
&dagger;project lead&emsp;&emsp;&emsp;*corresponding author

<a href="https://arxiv.org/abs/2412.04332"><img src='https://img.shields.io/badge/arXiv-Liquid-red' alt='Paper PDF'></a>
<a href="https://foundationvision.github.io/Liquid/"><img src='https://img.shields.io/badge/Project_Page-Liquid-green' alt='Project Page'></a>
<a href="https://huggingface.co/Junfeng5/Liquid_V1_7B"><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a>
<a href="https://huggingface.co/spaces/Junfeng5/Liquid_demo"><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-red'></a>


</div>

<font size="4">This repo implements Liquid, a scalable and unified autoregressive generation paradigm that seamlessly integrates multimodal comprehension and generation.</font>

![teaser](assets/liquid_framework.png)

##  News

**2025-03-25:** Data processing and model pretraining scripts have been updated in [Data.md](./Data.md) and [TRAIN.md](./TRAIN.md).

**2025-03-04:** Text-to-image and visual understanding evaluation scripts for Liquid are released in [EVAL.md](evaluation/EVAL.md).

**2025-02-28:** Paper, demo, model, and [project page](https://foundationvision.github.io/Liquid/) for Liquid are all released.

##  Open-Source Plan
- Liquid-7B-IT (Instruction Tuned Multimodal Model with Instruction Following Ability)
  - [] Web Demo
  - [] Evaluation 
  - [] Checkpoints
  - [] Training Codes
- Liquid-0.5B~32B-Pretrain (Multimodal extension models of six different scales ranging from 0.5B to 32B across three model families. )
  - [ ] Checkpoints

# 斤Inference

Using Liquid for inference or evaluation doesn't require complex environment dependencies. Since it's essentially a HuggingFace format language model, you only need the `transformers` library and some basic components to run it. Refer to [EVAL.md](evaluation/EVAL.md) for recommended versions.

### Run the Gradio Demo locally

If deploying on a GPU with less than 30GB VRAM, you may need to enable `load_in_8bit` in `AutoModelForCausalLM.from_pretrained` in `app.py` for image generation to avoid out-of-memory errors.

```bash
pip install gradio==4.44.1
pip install gradio_client==1.3.0

cd evaluation
python app.py
```

### Single inference

```bash
# Engage in pure language dialogue.

python inference_t2t.py  --model_path Junfeng5/Liquid_V1_7B  --prompt  "Write me a poem about Machine Learning."


# image understanding
python inference_i2t.py --model_path Junfeng5/Liquid_V1_7B  --image_path samples/baklava.png   --prompt 'How to make this pastry?'


# image generation, add --load_8bit for GPU with less than 30GB VRAM
python inference_t2i.py   --model_path Junfeng5/Liquid_V1_7B --prompt "young blue dragon with horn lightning in the style of dd fantasy full body"  
```



## 锔 Installation and Training

See [Data.md](./Data.md) and [TRAIN.md](./TRAIN.md).




##  Introduction
* We present Liquid, an auto-regressive generation paradigm that **seamlessly integrates visual comprehension and generation.**

* Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using a single large language model (LLM), eliminating the need for external pretrained visual embeddings such as CLIP. 

* For the first time, Liquid uncovers a **scaling law** that performance drop unavoidably brought by the unified training of visual and language tasks diminishes as the model size increases.

* Furthermore, the unified token space enables visual generation and comprehension tasks to **mutually enhance each other**


##  Multimodal Generation
* <font size="4">Liquid : Scalable and Versatile Unified Multimodal Generator which supports Visual Understanding, Visual Generation and Multi-modal Generation</font>

![teaser](assets/multimodal_task.png)


* <font size="4">Liquid can generate high-quality, photorealistic images of any aspect ratio by language in an autoregressive paradigm.</font>

![teaser](assets/samples_multiratio.jpg)

##  Scaling Law for multimodal generation
* <font size="4">Liquid shows clear Scaling Law in multimodal generation across different sizes(0.5B to 32B).</font>

![teaser](assets/multimodal_scaling_law.png)


## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Citation

If you find this project useful, please consider citing:

```bibtex
@article{wu2024liquid,
  title={Liquid: Language models are scalable multi-modal generators},
  author={Wu, Junfeng and Jiang, Yi and Ma, Chuofan and Liu, Yuliang and Zhao, Hengshuang and Yuan, Zehuan and Bai, Song and Bai, Xiang},
  journal={arXiv preprint arXiv:2412.04332},
  year={2024}
}

```
